{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Network(Numpy).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBg_m-kByVtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple neural network using numpy \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.random.randn(3,4)\n",
        "y = np.random.randn(3,4)\n",
        "\n",
        "def sigmoid (x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def derivatives_sigmoid(x):\n",
        "  return x*(1-x)\n",
        "\n",
        "epoch = 5000\n",
        "lr = 0.1 # learning rate hyper parameter\n",
        "\n",
        "# number of fetures in the the dataset, hidden layer neurons, output layer \n",
        "inputlayer_neuron = x.shape[1] # shape returns a tuple with the m and n \n",
        "hiddenlayer_neurons = 3\n",
        "output_neurons = 1\n",
        "\n",
        "# weight and bias intialization \n",
        "wh = np.random.uniform(size =(inputlayer_neuron, hiddenlayer_neurons))\n",
        "bh = np.random.uniform(size =(1, hiddenlayer_neurons))\n",
        "wout = np.random.uniform(size =(hiddenlayer_neurons, output_neurons))\n",
        "bout = np.random.uniform(size =(1, output_neurons))\n",
        "\n",
        "# creates weight and bias matrices for computation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPVGen984OtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(epoch):\n",
        "# Forward propogation \n",
        "  hidden_layer_input1= np.dot(x,wh)\n",
        "  hidden_layer_input= hidden_layer_input1 + bh\n",
        "  \n",
        "  hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
        "  \n",
        "  output_layer_input1 = np.dot(hiddenlayer_activations,wout)\n",
        "  output_layer_input = output_layer_input1 + bout\n",
        "  \n",
        "  output = sigmoid(output_layer_input)\n",
        "\n",
        "# Backpropagation\n",
        "\n",
        "  E = y - output\n",
        "  slope_output_layer = derivatives_sigmoid(output)\n",
        "  slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
        "  d_output = E* slope_output_layer\n",
        "  Error_at_hidden_layer = np.dot(d_output, wout.T)\n",
        "  d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
        "  wout += hiddenlayer_activations.T.dot(d_output)* lr\n",
        "  bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
        "  wh += X.T.dot(d_hiddenlayer) *lr\n",
        "  bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
        "\n",
        "print('actual :\\n',y,'\\n')\n",
        "print('predicted :\\n',output)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}